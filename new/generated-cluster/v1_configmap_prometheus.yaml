apiVersion: v1
data:
  extra_rules.yml: ""
  prometheus.yml: "global:\n  scrape_interval:     30s\n  evaluation_interval: 30s\n\nalerting:\n
    \ alertmanagers:\n    # Bundled Alertmanager, started by prom-wrapper\n    - static_configs:\n
    \       - targets: ['127.0.0.1:9093']\n      path_prefix: /alertmanager\n    #
    Uncomment the following to have alerts delivered to additional Alertmanagers discovered\n
    \   # in the cluster. This configuration is not required if you use Sourcegraph's
    built-in alerting:\n    # https://docs.sourcegraph.com/admin/observability/alerting\n
    \   # - kubernetes_sd_configs:\n    #  - role: endpoints\n    #  relabel_configs:\n
    \   #    - source_labels: [__meta_kubernetes_service_name]\n    #      regex:
    alertmanager\n    #      action: keep\n\nrule_files:\n  - '*_rules.yml'\n  - \"/sg_config_prometheus/*_rules.yml\"\n
    \ - \"/sg_prometheus_add_ons/*_rules.yml\"\n\n# A scrape configuration for running
    Prometheus on a Kubernetes cluster.\n# This uses separate scrape configs for cluster
    components (i.e. API server, node)\n# and services to allow each to use different
    authentication configs.\n#\n# Kubernetes labels will be added as Prometheus labels
    on metrics via the\n# `labelmap` relabeling action.\n\n# Scrape config for API
    servers.\n#\n# Kubernetes exposes API servers as endpoints to the default/kubernetes\n#
    service so this uses `endpoints` role and uses relabelling to only keep\n# the
    endpoints associated with the default/kubernetes service using the\n# default
    named port `https`. This works for single API server deployments as\n# well as
    HA API server deployments.\nscrape_configs:\n- job_name: 'kubernetes-apiservers'\n\n
    \ kubernetes_sd_configs:\n  - role: endpoints\n\n  # Default to scraping over
    https. If required, just disable this or change to\n  # `http`.\n  scheme: https\n\n
    \ # This TLS & bearer token file config is used to connect to the actual scrape\n
    \ # endpoints for cluster components. This is separate to discovery auth\n  #
    configuration because discovery & scraping are two separate concerns in\n  # Prometheus.
    The discovery auth config is automatic if Prometheus runs inside\n  # the cluster.
    Otherwise, more config options have to be provided within the\n  # <kubernetes_sd_config>.\n
    \ tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n
    \   # If your node certificates are self-signed or use a different CA to the\n
    \   # master CA, then disable certificate verification below. Note that\n    #
    certificate verification is an integral part of a secure infrastructure\n    #
    so this should only be disabled in a controlled environment. You can\n    # disable
    certificate verification by uncommenting the line below.\n    #\n    # insecure_skip_verify:
    true\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n
    \ # Keep only the default/kubernetes service endpoints for the https port. This\n
    \ # will add targets for each API server which Kubernetes adds an endpoint to\n
    \ # the default/kubernetes service.\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_namespace,
    __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n    action:
    keep\n    regex: default;kubernetes;https\n\n- job_name: 'kubernetes-nodes'\n\n
    \ # Default to scraping over https. If required, just disable this or change to\n
    \ # `http`.\n  scheme: https\n\n  # This TLS & bearer token file config is used
    to connect to the actual scrape\n  # endpoints for cluster components. This is
    separate to discovery auth\n  # configuration because discovery & scraping are
    two separate concerns in\n  # Prometheus. The discovery auth config is automatic
    if Prometheus runs inside\n  # the cluster. Otherwise, more config options have
    to be provided within the\n  # <kubernetes_sd_config>.\n  tls_config:\n    ca_file:
    /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    # If your node certificates
    are self-signed or use a different CA to the\n    # master CA, then disable certificate
    verification below. Note that\n    # certificate verification is an integral part
    of a secure infrastructure\n    # so this should only be disabled in a controlled
    environment. You can\n    # disable certificate verification by uncommenting the
    line below.\n    #\n    insecure_skip_verify: true\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n
    \ kubernetes_sd_configs:\n  - role: node\n\n  relabel_configs:\n  - action: labelmap\n
    \   regex: __meta_kubernetes_node_label_(.+)\n  - target_label: __address__\n
    \   replacement: kubernetes.default.svc:443\n  - source_labels: [__meta_kubernetes_node_name]\n
    \   regex: (.+)\n    target_label: __metrics_path__\n    replacement: /api/v1/nodes/${1}/proxy/metrics\n\n#
    Scrape config for service endpoints.\n#\n# The relabeling allows the actual service
    scrape endpoint to be configured\n# via the following annotations:\n#\n# * `sourcegraph.prometheus/scrape`:
    Only scrape services that have a value of `true`\n# * `prometheus.io/scheme`:
    If the metrics endpoint is secured then you will need\n# to set this to `https`
    & most likely set the `tls_config` of the scrape config.\n# * `prometheus.io/path`:
    If the metrics path is not `/metrics` override this.\n# * `prometheus.io/port`:
    If the metrics are exposed on a different port to the\n# service then set this
    appropriately.\n- job_name: 'kubernetes-service-endpoints'\n\n  kubernetes_sd_configs:\n
    \ - role: endpoints\n\n  relabel_configs:\n    # Sourcegraph specific customization,
    only scrape pods with our annotation\n  - source_labels: [__meta_kubernetes_service_annotation_sourcegraph_prometheus_scrape]\n
    \   action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n
    \   action: replace\n    target_label: __scheme__\n    regex: (https?)\n  - source_labels:
    [__meta_kubernetes_service_annotation_prometheus_io_path]\n    action: replace\n
    \   target_label: __metrics_path__\n    regex: (.+)\n  - source_labels: [__address__,
    __meta_kubernetes_service_annotation_prometheus_io_port]\n    action: replace\n
    \   target_label: __address__\n    regex: (.+)(?::\\d+);(\\d+)\n    replacement:
    $1:$2\n  - action: labelmap\n    regex: __meta_kubernetes_service_label_(.+)\n
    \ - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    # Sourcegraph
    specific customization. We want a more convenient to type label.\n    # target_label:
    kubernetes_namespace\n    target_label: ns\n  - source_labels: [__meta_kubernetes_service_name]\n
    \   action: replace\n    target_label: kubernetes_name\n  # Sourcegraph specific
    customization. We want a nicer name for job\n  - source_labels: [app]\n    action:
    replace\n    target_label: job\n  # Sourcegraph specific customization. We want
    a nicer name for instance\n  - source_labels: [__meta_kubernetes_pod_name]\n    action:
    replace\n    target_label: instance\n  # Sourcegraph specific customization. We
    want to add a label to every \n  # metric that indicates the node it came from.\n
    \ - source_labels: [__meta_kubernetes_endpoint_node_name]\n    action: replace\n
    \   target_label: nodename\n  metric_relabel_configs:\n  # Sourcegraph specific
    customization. Drop metrics with empty nodename responses from the k8s API\n  -
    source_labels: [nodename]\n    regex: ^$\n    action: drop\n\n# Example scrape
    config for probing services via the Blackbox Exporter.\n#\n# The relabeling allows
    the actual service scrape endpoint to be configured\n# via the following annotations:\n#\n#
    * `prometheus.io/probe`: Only probe services that have a value of `true`\n- job_name:
    'kubernetes-services'\n\n  metrics_path: /probe\n  params:\n    module: [http_2xx]\n\n
    \ kubernetes_sd_configs:\n  - role: service\n\n  relabel_configs:\n  - source_labels:
    [__meta_kubernetes_service_annotation_prometheus_io_probe]\n    action: keep\n
    \   regex: true\n  - source_labels: [__address__]\n    target_label: __param_target\n
    \ - target_label: __address__\n    replacement: blackbox\n  - source_labels: [__param_target]\n
    \   target_label: instance\n  - action: labelmap\n    regex: __meta_kubernetes_service_label_(.+)\n
    \ - source_labels: [__meta_kubernetes_service_namespace]\n    # Sourcegraph specific
    customization. We want a more convenient to type label.\n    # target_label: kubernetes_namespace\n
    \   target_label: ns\n  - source_labels: [__meta_kubernetes_service_name]\n    target_label:
    kubernetes_name\n\n# Example scrape config for pods\n#\n# The relabeling allows
    the actual pod scrape endpoint to be configured via the\n# following annotations:\n#\n#
    * `sourcegraph.prometheus/scrape`: Only scrape pods that have a value of `true`\n#
    * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n#
    * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default
    of `9102`.\n- job_name: 'kubernetes-pods'\n\n  kubernetes_sd_configs:\n  - role:
    pod\n\n  relabel_configs:\n    # Sourcegraph specific customization, only scrape
    pods with our annotation\n  - source_labels: [__meta_kubernetes_pod_annotation_sourcegraph_prometheus_scrape]\n
    \   action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n
    \   action: replace\n    target_label: __metrics_path__\n    regex: (.+)\n  -
    source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n
    \   action: replace\n    regex: (.+):(?:\\d+);(\\d+)\n    replacement: ${1}:${2}\n
    \   target_label: __address__\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n
    \ - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n    target_label:
    kubernetes_pod_name\n  # Sourcegraph specific customization. We want a more convenient
    to type label.\n  # target_label: kubernetes_namespace\n  - source_labels: [__meta_kubernetes_namespace]\n
    \   action: replace\n    target_label: ns\n  # Sourcegraph specific customization.
    We want to add a label to every \n  # metric that indicates the node it came from.\n
    \ - source_labels: [__meta_kubernetes_pod_node_name]\n    action: replace\n    target_label:
    nodename\n\n  metric_relabel_configs:\n  # cAdvisor-specific customization. Drop
    container metrics exported by cAdvisor\n  # not in the same namespace as Sourcegraph.\n
    \ # Uncomment this if you have problems with certain dashboards or cAdvisor itself\n
    \ # picking up non-Sourcegraph services. Ensure all Sourcegraph services are running\n
    \ # within the Sourcegraph namespace you have defined.\n  # The regex must keep
    matches on '^$' (empty string) to ensure other metrics do not\n  # get dropped.\n
    \ # - source_labels: [container_label_io_kubernetes_pod_namespace]\n  #   regex:
    ^$|ns-sourcegraph # ensure this matches with namespace declarations\n  #   action:
    keep\n  # cAdvisor-specific customization. We want container metrics to be named
    after their container name label.\n  # Note that 'io.kubernetes.container.name'
    and 'io.kubernetes.pod.name' must be provided in cAdvisor\n  # '--whitelisted_container_labels'
    (see cadvisor.DaemonSet.yaml)\n  - source_labels: [container_label_io_kubernetes_container_name,
    container_label_io_kubernetes_pod_name]\n    regex: (.+)\n    action: replace\n
    \   target_label: name\n    separator: '-'\n  # Sourcegraph specific customization.
    Drop metrics with empty nodename responses from the k8s API\n  - source_labels:
    [nodename]\n    regex: ^$\n    action: drop\n\n# Scrape prometheus itself for
    metrics.\n- job_name: 'builtin-prometheus'\n  static_configs:\n    - targets:
    ['127.0.0.1:9092']\n      labels:\n        app: prometheus\n- job_name: 'builtin-alertmanager'\n
    \ metrics_path: /alertmanager/metrics\n  static_configs:\n    - targets: ['127.0.0.1:9093']\n
    \     labels:\n        app: alertmanager\n"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: prometheus
    deploy: sourcegraph
    sourcegraph-resource-requires: no-cluster-admin
  name: prometheus
  namespace: sourcegraph
